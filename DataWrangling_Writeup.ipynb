{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Data Wrangling\n",
    "#### Springboard Data Science Career Track\n",
    "##### Tamara Monge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tami/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1345053 entries, 0 to 1345052\n",
      "Data columns (total 21 columns):\n",
      "Citation           1345053 non-null int64\n",
      "Tag                1344837 non-null object\n",
      "ExpMM              1300960 non-null object\n",
      "ExpYY              1345047 non-null float64\n",
      "State              1345053 non-null object\n",
      "Make               1343712 non-null object\n",
      "Address            1345051 non-null object\n",
      "ViolCode           1344668 non-null float64\n",
      "Description        1345053 non-null object\n",
      "ViolFine           1344668 non-null object\n",
      "ViolDate           1340590 non-null object\n",
      "Balance            1345053 non-null object\n",
      "PenaltyDate        0 non-null float64\n",
      "OpenFine           1344668 non-null object\n",
      "OpenPenalty        1344668 non-null object\n",
      "NoticeDate         602001 non-null object\n",
      "ImportDate         1345053 non-null object\n",
      "Neighborhood       206670 non-null object\n",
      "PoliceDistrict     206670 non-null object\n",
      "CouncilDistrict    206691 non-null float64\n",
      "Location           1323450 non-null object\n",
      "dtypes: float64(4), int64(1), object(16)\n",
      "memory usage: 215.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "df_full = pd.read_csv ('/Users/tami/Documents/Springboard/Capstone1/Parking_Citations.csv')\n",
    "df_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the initial DataFrame was composed of 21 columns: 16 object Series (Tag, ExpMM, State, Make, Address, Description, ViolFine, ViolDate, Balance, OpenFine, OpenPenalty, NoticeDate, ImportDate, Neighborhood, PoliceDistrict, Location), 4 float64 Series (ExpYY, ViolCode, PenaltyDate, CouncilDistrict) and 1 int64 Series (Citation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Remove irrelevant cohort\n",
    "The initial DataFrame contained two distinct cohorts of data: (1) all citations issued over the past two years and (2) any citations issued more than two years ago that still have an open balance due. Since this study is concerned with the former cohort, the first data wrangling step was to convert the ViolDate column to a datetime object and use it to extract the first cohort into its own DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 753387 entries, 5 to 1345052\n",
      "Data columns (total 21 columns):\n",
      "Citation           753387 non-null int64\n",
      "Tag                753260 non-null object\n",
      "ExpMM              725018 non-null object\n",
      "ExpYY              753387 non-null float64\n",
      "State              753387 non-null object\n",
      "Make               752981 non-null object\n",
      "Address            753387 non-null object\n",
      "ViolCode           753387 non-null float64\n",
      "Description        753387 non-null object\n",
      "ViolFine           753387 non-null object\n",
      "ViolDate           753387 non-null datetime64[ns]\n",
      "Balance            753387 non-null object\n",
      "PenaltyDate        0 non-null float64\n",
      "OpenFine           753387 non-null object\n",
      "OpenPenalty        753387 non-null object\n",
      "NoticeDate         171237 non-null object\n",
      "ImportDate         753387 non-null object\n",
      "Neighborhood       123652 non-null object\n",
      "PoliceDistrict     123652 non-null object\n",
      "CouncilDistrict    123661 non-null float64\n",
      "Location           743041 non-null object\n",
      "dtypes: datetime64[ns](1), float64(4), int64(1), object(15)\n",
      "memory usage: 126.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_full['ViolDate'] = df_full['ViolDate'][pd.notnull(df_full['ViolDate'])].apply(lambda x: datetime.strptime(x, '%m/%d/%Y %I:%M:%S %p'))\n",
    "df = df_full[:][df_full['ViolDate'] >= '2015-09-23']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame consists of 753,387 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Drop irrelevant columns\n",
    "The next data wrangling step was to drop the irrelevant columns: ExpMM, ExpYY, PenaltyDate, NoticeDate, ImportDate, Neighborhood, PoliceDistrict, and CouncilDistrict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(['ExpMM', 'ExpYY', 'PenaltyDate', 'NoticeDate', 'ImportDate', 'Neighborhood', 'PoliceDistrict', 'CouncilDistrict'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Clean the columns\n",
    "Next, we needed to clean the individual columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Unaltered Columns\n",
    "Five columns (Citation, Tag, ViolCode, Description, Address) required no cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Adding year, month, day, and hour Columns \n",
    "Using the ViolDate column of datetime objects, we saved the year, month, day, and hour as new columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['year']  = df['ViolDate'][pd.notnull(df['ViolDate'])].dt.year.astype(int)\n",
    "df['month'] = df['ViolDate'][pd.notnull(df['ViolDate'])].dt.month.astype(int)\n",
    "df['day']   = df['ViolDate'][pd.notnull(df['ViolDate'])].dt.day.astype(int)\n",
    "df['hour']  = df['ViolDate'][pd.notnull(df['ViolDate'])].dt.hour.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) Financial Columns\n",
    "Four columns (ViolFine, Balance, OpenFine, and OpenPenalty) contained financial data in string format that needed to be converted to floats. This process required 3 cleaning steps: removing null values, applying a lambda function to trim the '$' and converting the strings to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['ViolFine']    = df['ViolFine'][df['ViolFine'].apply(type) == str].apply(lambda x: x[1:]).astype(float) \n",
    "df['Balance']     = df['Balance'][df['Balance'].apply(type) == str].apply(lambda x: x[1:]).astype(float)   \n",
    "df['OpenFine']    = df['OpenFine'][df['OpenFine'].apply(type) == str].apply(lambda x: x[1:]).astype(float)   \n",
    "df['OpenPenalty'] = df['OpenPenalty'][df['OpenPenalty'].apply(type) == str].apply(lambda x: x[1:]).astype(float)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D) String Columns \n",
    "Three of the columns (Address, State, and Make) contained strings with inconsistent cases that needed to be standardized. In addition, the Make column contained strings of inconsistent lengths to represent the same category (e.g., 'Hon' and 'HONDA') and thus needed the string length needed to be standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Address'] = df['Address'].str.upper()\n",
    "df['State'] = df['State'].str.upper()\n",
    "df['Make']  = df['Make'].str.upper()\n",
    "df['Make']  = df['Make'][df['Make'].apply(type) == str].apply(lambda x: x[:3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E) Dissect the Location Column into latitude, longitude, street_address, and latlon Columns\n",
    "The Location column contained strings that housed multiple pieces of geographic information (e.g., '6000 CHINQUAPIN PKWY\\nBaltimore, MD\\n(39.365093, -76.59764)') and thus required two cleaning steps: writing and applying functions to split the strings into their constituent parts and save those parts as new columns (latitude, longitude, street_address, latlon) in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latitude(location):\n",
    "    \"\"\"Ingest Location and return latitude\"\"\"\n",
    "    lat = float('NaN')\n",
    "    if type(location) == str:\n",
    "        if ((len(location.split('\\n')) > 2) and (location.split('\\n')[2] != '')):\n",
    "            lat = float(location.split('\\n')[2].split('(')[1].split(',')[0])\n",
    "    return lat\n",
    "\n",
    "\n",
    "def longitude(location):\n",
    "    \"\"\"Ingest Location and return longitude\"\"\"\n",
    "    lon = float('NaN')\n",
    "    if type(location) == str:\n",
    "        if ((len(location.split('\\n')) > 2) and (location.split('\\n')[2] != '')):\n",
    "            lon = float(location.split('\\n')[2].split('(')[1].split(',')[1].split(')')[0])       \n",
    "    return lon\n",
    "\n",
    "\n",
    "def street_address(location):\n",
    "    \"\"\"Ingest Location and return street address\"\"\"\n",
    "    address = ''\n",
    "    if type(location) == str and len(location.split('\\n')) >=1 :\n",
    "        address = location.split('\\n')[0]       \n",
    "    return address\n",
    "\n",
    "\n",
    "def latlon(location):\n",
    "    \"\"\"Ingest Location and return tuple of latitude, longitude\"\"\"\n",
    "    latlon = ''\n",
    "    if type(location) == str and ((len(location.split('\\n')) > 2) and (location.split('\\n')[2] != '')):\n",
    "            latlon = (location.split('\\n')[2])\n",
    "    return latlon\n",
    "\n",
    "\n",
    "df['latitude']       = df['Location'].apply(latitude)\n",
    "df['longitude']      = df['Location'].apply(longitude)\n",
    "df['street_address'] = df['Location'].apply(street_address)\n",
    "df['latlon']         = df['Location'].apply(latlon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Set the DateTimeIndex\n",
    "Next, we set the ViolDate column to be the DateTimeIndex of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_full.index = df_full['ViolDate']\n",
    "df.drop('ViolDate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Handle Missing Data\n",
    "Once the dataset was clean, the next step in data wrangling was to deal with missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 753387 entries, 5 to 1345052\n",
      "Data columns (total 20 columns):\n",
      "Citation          753387 non-null int64\n",
      "Tag               753260 non-null object\n",
      "State             753387 non-null object\n",
      "Make              752981 non-null object\n",
      "Address           753387 non-null object\n",
      "ViolCode          753387 non-null float64\n",
      "Description       753387 non-null object\n",
      "ViolFine          753387 non-null float64\n",
      "Balance           753387 non-null float64\n",
      "OpenFine          753387 non-null float64\n",
      "OpenPenalty       753387 non-null float64\n",
      "Location          743041 non-null object\n",
      "year              753387 non-null int64\n",
      "month             753387 non-null int64\n",
      "day               753387 non-null int64\n",
      "hour              753387 non-null int64\n",
      "latitude          529579 non-null float64\n",
      "longitude         529579 non-null float64\n",
      "street_address    753387 non-null object\n",
      "latlon            753387 non-null object\n",
      "dtypes: float64(7), int64(5), object(8)\n",
      "memory usage: 140.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Drop Nulls\n",
    "\n",
    "As shown above, the DataFrame now consists of 20 columns and 753,387 rows. 17 of the columns (all except latitude, longitude, and latlon) contain <= 1.5% missing data. Given this low percentage of missing data, we found it acceptable to simply drop the observations that contained missing values in one or more of these 17 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 742513 entries, 5 to 1345052\n",
      "Data columns (total 20 columns):\n",
      "Citation          742513 non-null int64\n",
      "Tag               742513 non-null object\n",
      "State             742513 non-null object\n",
      "Make              742513 non-null object\n",
      "Address           742513 non-null object\n",
      "ViolCode          742513 non-null float64\n",
      "Description       742513 non-null object\n",
      "ViolFine          742513 non-null float64\n",
      "Balance           742513 non-null float64\n",
      "OpenFine          742513 non-null float64\n",
      "OpenPenalty       742513 non-null float64\n",
      "Location          742513 non-null object\n",
      "year              742513 non-null int64\n",
      "month             742513 non-null int64\n",
      "day               742513 non-null int64\n",
      "hour              742513 non-null int64\n",
      "latitude          529161 non-null float64\n",
      "longitude         529161 non-null float64\n",
      "street_address    742513 non-null object\n",
      "latlon            742513 non-null object\n",
      "dtypes: float64(7), int64(5), object(8)\n",
      "memory usage: 119.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.dropna(axis=0, how='any', subset=['Citation', 'Tag', 'State', 'Make', 'Address', 'ViolCode', 'Description', 'ViolFine', 'Balance', 'OpenFine', 'OpenPenalty', 'Location', 'year', 'month', 'day', 'hour', 'street_address'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This left us with 742,513 observations or 98.6% of the original dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Dealing with missing data in the latitude, longitude, and latlon columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.266226988618385"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "71.266226988618385"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "71.266226988618385"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['latitude'].notnull().sum()/df['Citation'].count()*100\n",
    "df['longitude'].notnull().sum()/df['Citation'].count()*100\n",
    "df['Citation'][df['latlon'] != ''].count()/df['Citation'].count()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 742,513 observations, only 71.3% contained values in the latlon, latitude, and longitude columns. We attempted to remedy this by creating a dictionary with key-value pairs of Address:latlon and using the dictionary to populate a new_latlon column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71585547997139443"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_latlon = zip(df['Address'], df['latlon'])\n",
    "address_dict = dict(address_latlon)\n",
    "df['new_latlon'] = df['Address'].map(address_dict)\n",
    "df['Citation'][df['new_latlon'] != ''].count()/df['Citation'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted in a very small success rate, however: 0.3 % increase in poplulated observations. Consequently, we must be reserved in our interpretation of any geographic trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Reorder and rename columns\n",
    "The final data wrangling step requried was to reorder and rename the columns to convenient names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Citation','Tag', 'State', 'Make', 'year', 'month', 'day', 'hour', 'ViolCode', 'Description', 'Address','new_latlon', 'latitude', 'longitude',  'ViolFine', 'Balance', 'OpenFine', 'OpenPenalty']]\n",
    "df.columns =['citation', 'tag', 'state', 'make', 'year', 'month', 'day', 'hour', 'code', 'description', 'address', 'latlon', 'latitude', 'longitude', 'fine', 'balance', 'open_fine', 'open_penalty']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
